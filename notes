multipass launch --name k3sMasterNode --cpus=2 --memory=4G --disk=5G
multipass launch --name k3sWorkerNode1 --cpus=2 --memory=4G --disk=5G

multipass exec k3sMasterNode -- /bin/bash -c "curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=644 sh -"
K3S_MASTERNODE_IP="https://$(multipass info k3sMasterNode | grep "IPv4" | awk -F' ' '{print $2}'):6443"

TOKEN="$(multipass exec k3sMasterNode -- /bin/bash -c "sudo cat /var/lib/rancher/k3s/server/node-token")"

multipass exec k3sWorkerNode1 -- /bin/bash -c "curl -sfL https://get.k3s.io | K3S_TOKEN=${TOKEN} K3S_URL=${K3S_MASTERNODE_IP} sh -"


multipass exec k3s-lead -- bash


kubectl create deployment nginx --image=nginx
kubectl expose deployment nginx --port=80 --type=ClusterIP

kubectl get svc nginx


multipass exec k3s-lead -- bash -c "
export KUBECONFIG=/etc/rancher/k3s/k3s.yaml && \
helm install cilium cilium/cilium \
  --namespace kube-system \
  --set kubeProxyReplacement=true \
  --set k8sServiceHost=192.168.65.25 \
  --set k8sServicePort=6443 \
  --set ipam.mode=kubernetes
"


HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)
HUBBLE_ARCH=arm64
if [ "$(uname -m)" = "aarch64" ]; then HUBBLE_ARCH=arm64; fi

curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/${HUBBLE_VERSION}/hubble-linux-${HUBBLE_ARCH}.tar.gz{,.sha256sum}
shasum -a 256 -c hubble-linux-${HUBBLE_ARCH}.tar.gz.sha256sum
sudo tar xzvfC hubble-linux-${HUBBLE_ARCH}.tar.gz /usr/local/bin
rm hubble-linux-${HUBBLE_ARCH}.tar.gz{,.sha256sum}

export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
helm upgrade cilium cilium/cilium --reuse-values \
  --namespace kube-system \
  --set hubble.enabled=true \
  --set hubble.relay.enabled=true \
  --set hubble.ui.enabled=true

export KUBECONFIG=/etc/rancher/k3s/k3s.yaml

echo 'export KUBECONFIG=/etc/rancher/k3s/k3s.yaml' >> ~/.bashrc
source ~/.bashrc


kubectl -n kube-system patch configmap cilium-config --type merge -p '{"data":{"enable-policy":"always"}}'

sudo lsof -i :4245

sudo cat /var/lib/rancher/k3s/server/node-token

kubectl get nodes -o wide

kubectl get pods -A

-- POLICIES COMMANDS

kubectl apply -f deploy.yaml -f deny-policy.yaml -f service.yaml

kubectl delete -f deny-policy.yaml

POD_A=$(kubectl get pod -n test-a -l app=pod-a -o jsonpath='{.items[0].metadata.name}')

kubectl exec -n test-a pod/pod-a-788b6584b9-65wwz -- curl -m 5 -s pod-b.test-b.svc.cluster.local

kubectl get cnp -n test-b



echo "Installing Cilium using Helm and Docker image..."
run_on_k3s_lead "
helm repo add cilium https://helm.cilium.io && \
helm repo update && \
helm install cilium cilium/cilium \
  --namespace kube-system \
  --set kubeProxyReplacement=true \
  --set k8sServiceHost=$IP \
  --set k8sServicePort=6443 \
  --set ipam.mode=kubernetes \
  --set image.repository=quay.io/cilium/cilium \
  --set image.tag=latest
" || error "Failed to install Cilium"


TO BUILD IMAGES ON VM

export DOCKER_BUILDKIT=1 && \
export DOCKER_BUILDX=1 && \
export ARCH=arm64 && \
export DOCKER_REGISTRY=docker.io && \
export DOCKER_DEV_ACCOUNT=tmmymrtnez && \
export DOCKER_IMAGE_TAG=latest && \
make dev-docker-image
make docker-operator-generic-imagede


TO INSTALL OPERATOR

helm upgrade cilium cilium/cilium \
  --namespace kube-system \
  --set operator.image.repository=null \
  --set operator.image.tag=null \
  --set operator.image.digest=null \
  --set operator.image.override=null \
  --set operator.image.useDigest=false \
  --set operator.replicas=null \
  --set operator.resources=null \
  --set operator.nodeSelector=null \
  --set operator.affinity=null \
  --set operator.tolerations=null \
  --set operator.priorityClassName=null \
  --set operator.image.pullPolicy=null

kubectl scale deployment cilium-operator -n kube-system --replicas=0
sleep 5
helm upgrade cilium cilium/cilium \
  --namespace kube-system \
  --reuse-values \
  --set operator.image.override="docker.io/tmmymrtnez/operator-generic:latest" \
  --set operator.image.useDigest=false \
  --set operator.image.pullPolicy=IfNotPresent \
  --set operator.replicas=2
kubectl scale deployment cilium-operator -n kube-system --replicas=2


helm upgrade cilium cilium/cilium \
  --namespace kube-system \
  --reuse-values \
  --set image.repository="docker.io/tmmymrtnez/cilium-dev" \
  --set image.tag="latest" \
  --set image.pullPolicy=IfNotPresent \
  --set image.useDigest=false






# Add centralized logging setup section with Loki and Promtail
echo "Setting up centralized log storage with Loki and Promtail..."

# Create a directory on k3s-lead for Loki storage
echo "Creating log storage directory on k3s-lead..."
multipass exec k3s-lead -- bash -c "sudo mkdir -p /var/log/loki && sudo chmod 755 /var/log/loki" || error "Failed to create Loki storage directory"

# Create a PersistentVolume (PV) and PersistentVolumeClaim (PVC) for Loki storage
echo "Creating PersistentVolume and PersistentVolumeClaim for Loki storage..."
run_on_k3s_lead "
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: loki-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /var/log/loki
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - k3s-lead
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: loki-pvc
  namespace: kube-system
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: local-storage
EOF
" || error "Failed to create PV/PVC for Loki storage"

# Install Loki using Helm with corrected PVC and strict SingleBinary mode
echo "Installing Loki using Helm..."
run_on_k3s_lead "
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
helm uninstall loki -n kube-system || true  # Clean up previous failed install
helm install loki grafana/loki \
  --namespace kube-system \
  --set deploymentMode=SingleBinary \
  --set singleBinary.replicas=1 \
  --set singleBinary.persistence.enabled=true \
  --set singleBinary.persistence.storageClass=local-storage \
  --set singleBinary.persistence.size=10Gi \
  --set singleBinary.persistence.existingClaim=loki-pvc \
  --set singleBinary.persistence.persistentVolumeClaimRetentionPolicy.enabled=false \
  --set singleBinary.resources.requests.cpu=100m \
  --set singleBinary.resources.requests.memory=256Mi \
  --set singleBinary.nodeSelector."kubernetes\.io/hostname"=k3s-lead \
  --set service.type=ClusterIP \
  --set loki.storage.type=filesystem \
  --set loki.commonConfig.replication_factor=1 \
  --set loki.schemaConfig.configs[0].from='2025-03-04' \
  --set loki.schemaConfig.configs[0].store=boltdb-shipper \
  --set loki.schemaConfig.configs[0].object_store=filesystem \
  --set loki.schemaConfig.configs[0].schema=v11 \
  --set loki.schemaConfig.configs[0].index.period=24h \
  --set loki.storage_config.boltdb_shipper.active_index_directory=/data/loki/index \
  --set loki.storage_config.boltdb_shipper.cache_location=/data/loki/boltdb-cache \
  --set loki.storage_config.filesystem.directory=/data/loki/chunks \
  --set read.replicas=0 \
  --set write.replicas=0 \
  --set backend.replicas=0 \
  --set chunksCache.enabled=false \
  --set chunksCache.replicas=0 \
  --set resultsCache.enabled=false \
  --set resultsCache.replicas=0 \
  --set gateway.enabled=false \
  --set monitoring.lokiCanary.enabled=false
" || error "Failed to install Loki"

# Wait for Loki to be ready
echo "Waiting for Loki to be ready..."
run_on_k3s_lead "kubectl wait --for=condition=ready pods -n kube-system -l app.kubernetes.io/name=loki --timeout=120s" || error "Failed to wait for Loki pod"

# Get Loki service ClusterIP
LOKI_IP=$(run_on_k3s_lead "kubectl get svc -n kube-system loki -o jsonpath='{.spec.clusterIP}'")
echo "Loki Service IP: $LOKI_IP"

# Install Promtail using Helm to collect logs from both nodes
echo "Installing Promtail to collect and send logs to Loki..."
run_on_k3s_lead "
helm install promtail grafana/promtail \
  --namespace kube-system \
  --set config.clients[0].url=http://$LOKI_IP:3100/loki/api/v1/push \
  --set config.snippets.pipelineStages[0].docker={} \
  --set config.snippets.extraScrapeConfigs[0].job_name=k3s \
  --set config.snippets.extraScrapeConfigs[0].pipeline_stages= \
  --set config.snippets.extraScrapeConfigs[0].static_configs[0].targets[0]=localhost \
  --set config.snippets.extraScrapeConfigs[0].static_configs[0].labels.job=k3s \
  --set config.snippets.extraScrapeConfigs[0].static_configs[0].labels.__path__=/var/log/*log \
  --set tolerations[0].key=\"node-role.kubernetes.io/master\" \
  --set tolerations[0].operator=\"Exists\" \
  --set tolerations[0].effect=\"NoSchedule\" \
  --set extraVolumeMounts[0].name=\"varlog\" \
  --set extraVolumeMounts[0].mountPath=\"/var/log\" \
  --set extraVolumes[0].name=\"varlog\" \
  --set extraVolumes[0].hostPath.path=\"/var/log\"
" || error "Failed to install Promtail"

# Wait for Promtail to be ready
echo "Waiting for Promtail to be ready..."
run_on_k3s_lead "kubectl wait --for=condition=ready pods -n kube-system -l app.kubernetes.io/name=promtail --timeout=120s" || error "Failed to wait for Promtail pods"

# Verify Loki and Promtail setup
echo "Verifying centralized log collection..."
run_on_k3s_lead "kubectl get pods -n kube-system -l app.kubernetes.io/name=loki"
run_on_k3s_lead "kubectl get pods -n kube-system -l app.kubernetes.io/name=promtail"

echo "Centralized log storage setup complete! Logs from both nodes are stored in Loki at /var/log/loki on k3s-lead."
echo "You can access logs by querying Loki at http://$LOKI_IP:3100 or by using Grafana."

echo "K3s cluster with Cilium, Hubble, Nginx, and Loki/Promtail setup is complete!"






# Add centralized logging setup section
echo "Setting up centralized log storage with rsyslog streaming to a log collector..."

# Create a new log-collector VM
echo "Creating log-collector VM..."
multipass launch --name log-collector --memory 1G --disk 5G || error "Failed to create log-collector VM"
LOG_COLLECTOR_IP=$(multipass info log-collector | grep 'IPv4' | awk '{print $2}') || error "Failed to retrieve log-collector IP"
echo "Log Collector IP: $LOG_COLLECTOR_IP"

# Install rsyslog on all nodes
echo "Installing rsyslog on k3s-lead, k3s-follower, and log-collector..."
multipass exec k3s-lead -- bash -c "sudo apt-get update && sudo apt-get install -y rsyslog" || error "Failed to install rsyslog on k3s-lead"
multipass exec k3s-follower -- bash -c "sudo apt-get update && sudo apt-get install -y rsyslog" || error "Failed to install rsyslog on k3s-follower"
multipass exec log-collector -- bash -c "sudo apt-get update && sudo apt-get install -y rsyslog" || error "Failed to install rsyslog on log-collector"

# Configure rsyslog to read from journald on k3s-lead and k3s-follower
echo "Configuring rsyslog to read from journald on k3s-lead..."
multipass exec k3s-lead -- bash -c "sudo bash -c 'echo \"\$ModLoad imjournal\" >> /etc/rsyslog.conf && echo \"\$ImJournalRatelimitInterval 0\" >> /etc/rsyslog.conf'" || error "Failed to enable journald on k3s-lead"

echo "Configuring rsyslog to read from journald on k3s-follower..."
multipass exec k3s-follower -- bash -c "sudo bash -c 'echo \"\$ModLoad imjournal\" >> /etc/rsyslog.conf && echo \"\$ImJournalRatelimitInterval 0\" >> /etc/rsyslog.conf'" || error "Failed to enable journald on k3s-follower"

# Configure k3s-lead to forward all logs to log-collector
echo "Configuring k3s-lead to forward logs to log-collector..."
multipass exec k3s-lead -- bash -c "echo '*.* @${LOG_COLLECTOR_IP}:514' | sudo tee /etc/rsyslog.d/90-forward.conf" || error "Failed to configure rsyslog forwarding on k3s-lead"
multipass exec k3s-lead -- bash -c "sudo systemctl restart rsyslog" || error "Failed to restart rsyslog on k3s-lead"

# Configure k3s-follower to forward all logs to log-collector
echo "Configuring k3s-follower to forward logs to log-collector..."
multipass exec k3s-follower -- bash -c "echo '*.* @${LOG_COLLECTOR_IP}:514' | sudo tee /etc/rsyslog.d/90-forward.conf" || error "Failed to configure rsyslog forwarding on k3s-follower"
multipass exec k3s-follower -- bash -c "sudo systemctl restart rsyslog" || error "Failed to restart rsyslog on k3s-follower"

# Configure log-collector to receive logs and write to a single file with hostname and timestamp
echo "Configuring log-collector to receive logs..."
multipass exec log-collector -- bash -c "sudo bash -c 'cat <<EOF > /etc/rsyslog.d/90-collect.conf
# Enable UDP syslog reception
\$ModLoad imudp
\$UDPServerRun 514

# Template with hostname and timestamp
\$template CentralLogFormat,\"[%\$HOSTNAME%] %timegenerated% %syslogtag% %msg%\\n\"

# Write all logs to a single file
*.* /var/log/central-logs.log;CentralLogFormat
EOF'" || error "Failed to configure rsyslog on log-collector"

# Create the central log file and restart rsyslog on log-collector
multipass exec log-collector -- bash -c "sudo touch /var/log/central-logs.log && sudo chmod 644 /var/log/central-logs.log" || error "Failed to create central log file"
multipass exec log-collector -- bash -c "sudo systemctl restart rsyslog" || error "Failed to restart rsyslog on log-collector"

# Ensure journald persists logs (optional, for reliability)
echo "Ensuring journald persists logs on k3s-lead and k3s-follower..."
multipass exec k3s-lead -- bash -c "sudo mkdir -p /var/log/journal && sudo systemctl restart systemd-journald" || error "Failed to persist journald on k3s-lead"
multipass exec k3s-follower -- bash -c "sudo mkdir -p /var/log/journal && sudo systemctl restart systemd-journald" || error "Failed to persist journald on k3s-follower"

# Wait for logs to start streaming
echo "Waiting 10 seconds for logs to start streaming..."
sleep 10

# Verify the combined log file
echo "Verifying centralized log collection..."
multipass exec log-collector -- bash -c "ls -l /var/log/central-logs.log" || error "Failed to list centralized log file"
multipass exec log-collector -- bash -c "tail -n 10 /var/log/central-logs.log" || error "Failed to read combined log file"

echo "Centralized log storage setup complete! All logs from both nodes are streaming to /var/log/central-logs.log on log-collector."
echo "View the combined logs with: multipass exec log-collector -- tail -f /var/log/central-logs.log"

echo "K3s cluster with Cilium, Hubble, Nginx, and centralized logging to log-collector is complete!"
